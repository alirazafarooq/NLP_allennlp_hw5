{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import Dict\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from allennlp.data.dataset_readers.stanford_sentiment_tree_bank import \\\n",
    "    StanfordSentimentTreeBankDatasetReader\n",
    "from allennlp.data.iterators import BucketIterator\n",
    "from allennlp.data.vocabulary import Vocabulary\n",
    "from allennlp.models import Model\n",
    "from allennlp.modules.seq2vec_encoders import Seq2VecEncoder, PytorchSeq2VecWrapper\n",
    "from allennlp.modules.text_field_embedders import TextFieldEmbedder, BasicTextFieldEmbedder\n",
    "from allennlp.modules.token_embedders import Embedding\n",
    "from allennlp.nn.util import get_text_field_mask\n",
    "from allennlp.training.metrics import CategoricalAccuracy, F1Measure\n",
    "from allennlp.training.trainer import Trainer\n",
    "\n",
    "from predictors import SentenceClassifierPredictor\n",
    "EMBEDDING_DIM = 128\n",
    "HIDDEN_DIM = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8544it [00:02, 3664.44it/s]\n",
      "1101it [00:00, 2886.73it/s]\n"
     ]
    }
   ],
   "source": [
    "reader = StanfordSentimentTreeBankDatasetReader()\n",
    "\n",
    "train_dataset = reader.read('data/stanfordSentimentTreebank/trees/train.txt')\n",
    "dev_dataset = reader.read('data/stanfordSentimentTreebank/trees/dev.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9645/9645 [00:00<00:00, 36274.35it/s]\n"
     ]
    }
   ],
   "source": [
    "vocab = Vocabulary.from_instances(train_dataset + dev_dataset,\n",
    "                                  min_count={'tokens': 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_embedding = Embedding(num_embeddings=vocab.get_vocab_size('tokens'),\n",
    "                            embedding_dim=EMBEDDING_DIM)\n",
    "# BasicTextFieldEmbedder takes a dict - we need an embedding just for tokens,\n",
    "# not for labels, which are used unchanged as the answer of the sentence classification\n",
    "word_embeddings = BasicTextFieldEmbedder({\"tokens\": token_embedding})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model in AllenNLP represents a model that is trained.\n",
    "class LstmClassifier(Model):\n",
    "    def __init__(self,\n",
    "                 word_embeddings: TextFieldEmbedder,\n",
    "                 encoder: Seq2VecEncoder,\n",
    "                 vocab: Vocabulary) -> None:\n",
    "        super().__init__(vocab)\n",
    "        # We need the embeddings to convert word IDs to their vector representations\n",
    "        self.word_embeddings = word_embeddings\n",
    "\n",
    "        # Seq2VecEncoder is a neural network abstraction that takes a sequence of something\n",
    "        # (usually a sequence of embedded word vectors), processes it, and returns it as a single\n",
    "        # vector. Oftentimes, this is an RNN-based architecture (e.g., LSTM or GRU), but\n",
    "        # AllenNLP also supports CNNs and other simple architectures (for example,\n",
    "        # just averaging over the input vectors).\n",
    "        self.encoder = encoder\n",
    "\n",
    "        # After converting a sequence of vectors to a single vector, we feed it into\n",
    "        # a fully-connected linear layer to reduce the dimension to the total number of labels.\n",
    "        self.hidden2tag = torch.nn.Linear(in_features=encoder.get_output_dim(),\n",
    "                                          out_features=vocab.get_vocab_size('labels'))\n",
    "        self.accuracy = CategoricalAccuracy()\n",
    "\n",
    "        # We use the cross-entropy loss because this is a classification task.\n",
    "        # Note that PyTorch's CrossEntropyLoss combines softmax and log likelihood loss,\n",
    "        # which makes it unnecessary to add a separate softmax layer.\n",
    "        self.loss_function = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # Instances are fed to forward after batching.\n",
    "    # Fields are passed through arguments with the same name.\n",
    "    def forward(self,\n",
    "                tokens: Dict[str, torch.Tensor],\n",
    "                label: torch.Tensor = None) -> torch.Tensor:\n",
    "        # In deep NLP, when sequences of tensors in different lengths are batched together,\n",
    "        # shorter sequences get padded with zeros to make them of equal length.\n",
    "        # Masking is the process to ignore extra zeros added by padding\n",
    "        mask = get_text_field_mask(tokens)\n",
    "\n",
    "        # Forward pass\n",
    "        embeddings = self.word_embeddings(tokens)\n",
    "        encoder_out = self.encoder(embeddings, mask)\n",
    "        logits = self.hidden2tag(encoder_out)\n",
    "\n",
    "        # In AllenNLP, the output of forward() is a dictionary.\n",
    "        # Your output dictionary must contain a \"loss\" key for your model to be trained.\n",
    "        output = {\"logits\": logits}\n",
    "        if label is not None:\n",
    "            self.accuracy(logits, label)\n",
    "            output[\"loss\"] = self.loss_function(logits, label)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = PytorchSeq2VecWrapper(\n",
    "    torch.nn.LSTM(EMBEDDING_DIM, HIDDEN_DIM, batch_first=True))\n",
    "\n",
    "model = LstmClassifier(word_embeddings, lstm, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 1.5788 ||: 100%|██████████| 267/267 [00:16<00:00, 15.71it/s]\n",
      "loss: 1.5757 ||: 100%|██████████| 35/35 [00:00<00:00, 59.71it/s]\n",
      "loss: 1.5656 ||: 100%|██████████| 267/267 [00:15<00:00, 16.80it/s]\n",
      "loss: 1.5723 ||: 100%|██████████| 35/35 [00:00<00:00, 57.55it/s]\n",
      "loss: 1.5588 ||: 100%|██████████| 267/267 [00:15<00:00, 16.99it/s]\n",
      "loss: 1.5677 ||: 100%|██████████| 35/35 [00:00<00:00, 67.73it/s]\n",
      "loss: 1.5384 ||: 100%|██████████| 267/267 [00:15<00:00, 16.72it/s]\n",
      "loss: 1.5582 ||: 100%|██████████| 35/35 [00:00<00:00, 66.28it/s]\n",
      "loss: 1.4750 ||: 100%|██████████| 267/267 [00:16<00:00, 14.35it/s]\n",
      "loss: 1.5267 ||: 100%|██████████| 35/35 [00:00<00:00, 53.23it/s]\n",
      "loss: 1.3601 ||: 100%|██████████| 267/267 [00:14<00:00, 20.11it/s]\n",
      "loss: 1.4941 ||: 100%|██████████| 35/35 [00:00<00:00, 78.50it/s] \n",
      "loss: 1.2145 ||: 100%|██████████| 267/267 [00:14<00:00, 18.45it/s]\n",
      "loss: 1.4841 ||: 100%|██████████| 35/35 [00:00<00:00, 75.86it/s]\n",
      "loss: 1.0755 ||: 100%|██████████| 267/267 [00:14<00:00, 18.42it/s]\n",
      "loss: 1.5301 ||: 100%|██████████| 35/35 [00:00<00:00, 76.71it/s] \n",
      "loss: 0.9598 ||: 100%|██████████| 267/267 [00:15<00:00, 15.86it/s]\n",
      "loss: 1.6022 ||: 100%|██████████| 35/35 [00:00<00:00, 66.42it/s]\n",
      "loss: 0.8802 ||: 100%|██████████| 267/267 [00:16<00:00, 16.50it/s]\n",
      "loss: 1.6190 ||: 100%|██████████| 35/35 [00:00<00:00, 64.87it/s]\n",
      "loss: 0.8073 ||: 100%|██████████| 267/267 [00:16<00:00, 16.62it/s]\n",
      "loss: 1.8391 ||: 100%|██████████| 35/35 [00:00<00:00, 65.44it/s]\n",
      "loss: 0.7462 ||: 100%|██████████| 267/267 [00:16<00:00, 16.19it/s]\n",
      "loss: 1.8280 ||: 100%|██████████| 35/35 [00:00<00:00, 52.95it/s]\n",
      "loss: 0.7020 ||: 100%|██████████| 267/267 [00:16<00:00, 16.27it/s]\n",
      "loss: 1.8877 ||: 100%|██████████| 35/35 [00:00<00:00, 61.37it/s]\n",
      "loss: 0.6606 ||: 100%|██████████| 267/267 [00:16<00:00, 16.46it/s]\n",
      "loss: 2.0950 ||: 100%|██████████| 35/35 [00:00<00:00, 66.89it/s]\n",
      "loss: 0.6154 ||: 100%|██████████| 267/267 [00:16<00:00, 17.39it/s]\n",
      "loss: 2.2051 ||: 100%|██████████| 35/35 [00:00<00:00, 64.49it/s]\n",
      "loss: 0.5821 ||: 100%|██████████| 267/267 [00:16<00:00, 14.99it/s]\n",
      "loss: 2.2444 ||: 100%|██████████| 35/35 [00:00<00:00, 59.09it/s]\n",
      "loss: 0.5410 ||: 100%|██████████| 267/267 [00:16<00:00, 16.27it/s]\n",
      "loss: 2.4307 ||: 100%|██████████| 35/35 [00:00<00:00, 60.45it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'training_duration': '00:04:22',\n",
       " 'training_start_epoch': 0,\n",
       " 'training_epochs': 15,\n",
       " 'epoch': 15,\n",
       " 'training_loss': 0.582126122847032,\n",
       " 'validation_loss': 2.244421570641654,\n",
       " 'best_epoch': 6,\n",
       " 'best_validation_loss': 1.4841412782669068}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "\n",
    "iterator = BucketIterator(batch_size=32, sorting_keys=[(\"tokens\", \"num_tokens\")])\n",
    "iterator.index_with(vocab)\n",
    "\n",
    "trainer = Trainer(model=model,\n",
    "                  optimizer=optimizer,\n",
    "                  iterator=iterator,\n",
    "                  train_dataset=train_dataset,\n",
    "                  validation_dataset=dev_dataset,\n",
    "                  patience=10,\n",
    "                  num_epochs=20)\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "tokens = ['This', 'is', 'the', 'best', 'movie', 'ever', '!']\n",
    "predictor = SentenceClassifierPredictor(model, dataset_reader=reader)\n",
    "logits = predictor.predict('This is the best movie ever!')['logits']\n",
    "label_id = np.argmax(logits)\n",
    "\n",
    "print(model.vocab.get_token_from_index(label_id, 'labels'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
